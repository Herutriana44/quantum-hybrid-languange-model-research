{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk gensim pennylane pennylane-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tHGb2m-mH2O",
        "outputId": "05940485-e7c5-4ed9-cd75-69cf1fbba9f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.40.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pennylane-lightning\n",
            "  Downloading PennyLane_Lightning-0.40.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m898.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.4.2)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.7.0)\n",
            "Collecting tomlkit (from pennylane)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (24.2)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning)\n",
            "  Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.1.31)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane-0.40.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.40.0-cp311-cp311-manylinux_2_28_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.1-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: appdirs, tomlkit, scipy-openblas32, numpy, autoray, scipy, rustworkx, diastatic-malt, gensim, pennylane-lightning, pennylane\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.1 diastatic-malt-2.15.2 gensim-4.3.3 numpy-1.26.4 pennylane-0.40.0 pennylane-lightning-0.40.0 rustworkx-0.16.0 scipy-1.13.1 scipy-openblas32-0.3.29.0.0 tomlkit-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import nltk\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msni_6c-lXle",
        "outputId": "3e13880e-317b-460a-dbd9-27d882a1270f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Word Embedding\n",
        "sentences = [\"Dynex powers Quantum entanglement\", \"Neuromorphic networks process qubits\"]\n",
        "tokenization = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "word2vec = Word2Vec(sentences=tokenization, vector_size=8, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "c2QeFYkBlZ_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Basis Embedding\n",
        "qubits = 8\n",
        "dev = qml.device(\"default.qubit\", wires=qubits)"
      ],
      "metadata": {
        "id": "jgQ58C36lb9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quantum Self-Attention Layers\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def QuantumSelfAttention(inputs, weights=np.random.randn(3, qubits, 3)):\n",
        "    # biIn = [1 if x >= 0 else 0 for x in inputs]\n",
        "    # qml.BasisEmbedding(biIn, wires=range(qubits))\n",
        "    qml.AngleEmbedding(inputs, wires=range(qubits))\n",
        "\n",
        "    \"\"\"\n",
        "    Rotation Gates: RX, RY, and RZ gates are applied to each qubit. These gates rotate the qubits around the X, Y, and Z axes respectively. The angles of rotation are determined by the values in the input vector. This step effectively processes each feature of the word embeddings, manipulating the quantum state to capture relationships between different dimensions.\n",
        "    Entanglement: CRZ and CNOT gates create entanglement between qubits, allowing the circuit to consider interactions between different features. Entanglement is a uniquely quantum phenomenon that links the states of qubits, so the state of one qubit depends on the state of another.\n",
        "    \"\"\"\n",
        "\n",
        "    for layer in range(3):\n",
        "        for i in range(qubits):\n",
        "            qml.RX(weights[layer][i][0] * inputs[i % len(inputs)], wires=i)\n",
        "            qml.RY(weights[layer][i][1] * inputs[(i + 1) % len(inputs)], wires=i)\n",
        "            qml.RZ(weights[layer][i][2] * inputs[(i + 2) % len(inputs)], wires=i)\n",
        "        for i in range(qubits - 1):\n",
        "            qml.CRZ(np.pi / (layer + 2), wires=[i, (i + 1) % qubits])\n",
        "            qml.CNOT(wires=[i, (i + 1) % qubits])\n",
        "\n",
        "    \"\"\"\n",
        "    Grover’s Operator\n",
        "    \"\"\"\n",
        "    qml.QFT(wires=range(qubits))\n",
        "    qml.adjoint(qml.QFT)(wires=range(qubits))\n",
        "    qml.GroverOperator(wires=range(qubits))\n",
        "\n",
        "    \"\"\"\n",
        "    Final Quantum Operations\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(qubits):\n",
        "        qml.Hadamard(wires=i)\n",
        "        qml.T(wires=i)\n",
        "        qml.RZ(inputs[i % len(inputs)], wires=i)\n",
        "\n",
        "        # qml.BasisEmbedding(biIn, wires=range(qubits))\n",
        "        qml.AngleEmbedding(inputs, wires=range(qubits))\n",
        "        return [qml.expval(qml.PauliZ(wires=i)) for i in range(qubits)]"
      ],
      "metadata": {
        "id": "_xs5P7eEleNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "def loss_fn(weights, input_vector, target_vector):\n",
        "    output = QuantumSelfAttention(input_vector, weights)\n",
        "\n",
        "    # Adjust dimension mismatch\n",
        "    if len(output) != len(target_vector):\n",
        "        adjusted = np.zeros_like(target_vector)\n",
        "        adjusted[:len(output)] = output\n",
        "    else:\n",
        "        adjusted = output\n",
        "\n",
        "    # Cosine distance (semakin kecil semakin bagus)\n",
        "    cos_sim = np.dot(adjusted, target_vector) / (np.linalg.norm(adjusted) * np.linalg.norm(target_vector))\n",
        "    return 1 - cos_sim\n",
        "\n",
        "\n",
        "def find_most_similar_word(vector):\n",
        "    \"\"\"Menemukan kata yang paling mirip dengan vektor yang diberikan\"\"\"\n",
        "    most_similar = None\n",
        "    max_similarity = -float('inf')\n",
        "\n",
        "    # Iterasi melalui semua kata dalam model Word2Vec\n",
        "    for word in word2vec.wv.index_to_key:\n",
        "        similarity = np.dot(word2vec.wv[word], vector) / (\n",
        "            np.linalg.norm(word2vec.wv[word]) * np.linalg.norm(vector))\n",
        "\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            most_similar = word\n",
        "\n",
        "    return most_similar\n",
        "\n",
        "def GenerateSentence(input_):\n",
        "    tokens = word_tokenize(input_.lower())\n",
        "    iEmbeddings = np.array([word2vec.wv[word] for word in tokens])\n",
        "    attOutputs = []\n",
        "\n",
        "    for embedding in iEmbeddings:\n",
        "        attOUT = QuantumSelfAttention(embedding)\n",
        "        softOUT = softmax(attOUT)\n",
        "        attOutputs.append(softOUT)\n",
        "\n",
        "    # Mengubah attOutputs menjadi kata-kata\n",
        "    generated_words = []\n",
        "    for vector in attOutputs:\n",
        "        # Gunakan attOUT sebagai vektor baru untuk mencari kata terdekat\n",
        "        # Karena attOUT adalah vektor hasil kuantum dengan dimensi 8 (jumlah qubit)\n",
        "        # kita perlu memastikan dimensinya cocok dengan word2vec\n",
        "        if len(vector) != word2vec.vector_size:\n",
        "            # Jika dimensi berbeda, potong atau tambah nilai untuk menyesuaikan\n",
        "            adjusted_vector = np.zeros(word2vec.vector_size)\n",
        "            for i in range(min(len(vector), word2vec.vector_size)):\n",
        "                adjusted_vector[i] = vector[i]\n",
        "        else:\n",
        "            adjusted_vector = vector\n",
        "\n",
        "        # Cari kata yang paling mirip dengan vektor yang dihasilkan\n",
        "        similar_word = find_most_similar_word(adjusted_vector)\n",
        "        generated_words.append(similar_word)\n",
        "\n",
        "    # Tambahkan beberapa kata dari kosakata untuk variasi jika diperlukan\n",
        "    if len(generated_words) < 4 and len(word2vec.wv.index_to_key) > 0:\n",
        "        additional_needed = 4 - len(generated_words)\n",
        "        # Ambil beberapa kata acak dari kosakata\n",
        "        import random\n",
        "        vocab_list = list(word2vec.wv.index_to_key)\n",
        "        for _ in range(min(additional_needed, len(vocab_list))):\n",
        "            generated_words.append(random.choice(vocab_list))\n",
        "\n",
        "    return ' '.join(generated_words[:5])"
      ],
      "metadata": {
        "id": "mHgOJVpKlhAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tes generated"
      ],
      "metadata": {
        "id": "xiK_yQGWNLks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ = np.random.choice(sentences)\n",
        "input_ = \"qubits\"\n",
        "generated = GenerateSentence(input_)\n",
        "print(f\"Generated: {generated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqIPtro4ljic",
        "outputId": "46acdc5a-a7ed-456b-ec0a-2d86e466fe4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: qubits networks neuromorphic entanglement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training"
      ],
      "metadata": {
        "id": "INnwZnHHNNb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "word2vec = Word2Vec(sentences=common_texts, vector_size=8, window=5, min_count=1, workers=1)\n"
      ],
      "metadata": {
        "id": "U_HRB9I_OAo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfbcFTa4PCKY",
        "outputId": "2f224e05-5af4-4d3f-eaa4-8710776a1baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human', 'interface', 'computer'],\n",
              " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'system'],\n",
              " ['system', 'human', 'system', 'eps'],\n",
              " ['user', 'response', 'time'],\n",
              " ['trees'],\n",
              " ['graph', 'trees'],\n",
              " ['graph', 'minors', 'trees'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    \"input\": [\"human\", \"computer\", \"system\", \"trees\"],\n",
        "    \"target\": [\"user\", \"system\", \"computer\", \"graph\"]\n",
        "})\n"
      ],
      "metadata": {
        "id": "nK_4g2oeNfEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Preprocess training data ===\n",
        "training_data = []\n",
        "missing_words = []\n",
        "\n",
        "for _, row in data.iterrows():\n",
        "    try:\n",
        "        input_vec = word2vec.wv[row[\"input\"]]\n",
        "        target_vec = word2vec.wv[row[\"target\"]]\n",
        "        training_data.append((input_vec, target_vec))\n",
        "    except KeyError as e:\n",
        "        missing_words.append(str(e))"
      ],
      "metadata": {
        "id": "gT6T315nOD-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pennylane.optimize import NesterovMomentumOptimizer\n",
        "\n",
        "num_layers = 3\n",
        "epochs = 10\n",
        "opt = AdamOptimizer(stepsize=0.01)\n",
        "weights = np.random.randn(num_layers, qubits, 3)  # initial weights\n",
        "opt = NesterovMomentumOptimizer(stepsize=0.01)\n",
        "\n",
        "if len(training_data) == 0:\n",
        "    print(\"❌ Tidak ada pasangan kata yang valid di Word2Vec! Berikut kata yang tidak ditemukan:\")\n",
        "    print(missing_words)\n",
        "else:\n",
        "    print(f\"✅ Total pasangan data untuk training: {len(training_data)}\")\n",
        "\n",
        "    # === Training ===\n",
        "    for epoch in range(epochs):\n",
        "        loss_total = 0\n",
        "        for input_vec, target_vec in training_data:\n",
        "            weights, loss = opt.step_and_cost(lambda w: loss_fn(w, input_vec, target_vec), weights)\n",
        "            loss_total += loss\n",
        "        print(f\"Epoch {epoch + 1} - Loss: {loss_total / len(training_data):.4f}\")"
      ],
      "metadata": {
        "id": "hC_ec2u-xBH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529eab1d-756c-4d9e-823f-273cbf1c5958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total pasangan data untuk training: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pennylane/_grad.py:216: UserWarning: Attempted to differentiate a function with no trainable parameters. If this is unintended, please add trainable parameters via the 'requires_grad' attribute or 'argnum' keyword.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 1.1197\n",
            "Epoch 2 - Loss: 1.1197\n",
            "Epoch 3 - Loss: 1.1197\n",
            "Epoch 4 - Loss: 1.1197\n",
            "Epoch 5 - Loss: 1.1197\n",
            "Epoch 6 - Loss: 1.1197\n",
            "Epoch 7 - Loss: 1.1197\n",
            "Epoch 8 - Loss: 1.1197\n",
            "Epoch 9 - Loss: 1.1197\n",
            "Epoch 10 - Loss: 1.1197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CHwwYcLAOwZE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}