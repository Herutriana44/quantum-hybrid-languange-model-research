# Quantum Hybrid Languange Model Research

## Description
This repository contains research related to integrating Quantum Computing with Large Language Models (LLMs). The primary focus of this project is on the development of *Quantum-Hybrid-LLMs*, which combines the power of models like GPT-2, T5-small, and Word2Vec with quantum layers to explore the potential improvements in natural language processing (NLP).

The addition of quantum layers is expected to bring advantages such as increased computational efficiency, faster data processing, and the potential for enhanced model capabilities in understanding complex semantic patterns.

## Objectives
The goals of this project include:
- Implementing quantum layers on large language models like GPT-2 and T5-small.
- Leveraging quantum principles to optimize word and sentence representations in Word2Vec.
- Exploring the combination of classical and quantum techniques in NLP to improve performance and efficiency.

## Structure Folder
- `notebook-research` : this folder contains all notebook research